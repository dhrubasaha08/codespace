{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "from dgllife.data import Tox21\n",
    "from dgllife.utils import SMILESToBigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer, RandomSplitter\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from dgl.data.utils import split_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score as rac\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from scipy import signal\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/7831\n",
      "Processing molecule 2000/7831\n",
      "Processing molecule 3000/7831\n",
      "Processing molecule 4000/7831\n",
      "Processing molecule 5000/7831\n",
      "Processing molecule 6000/7831\n",
      "Processing molecule 7000/7831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('CCOc1ccc2nc(S(N)(=O)=O)sc2c1',\n",
       " Graph(num_nodes=16, num_edges=34,\n",
       "       ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n",
       "       edata_schemes={'e': Scheme(shape=(12,), dtype=torch.float32)}),\n",
       " tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "smiles_to_g = SMILESToBigraph(node_featurizer=CanonicalAtomFeaturizer(), edge_featurizer=CanonicalBondFeaturizer())\n",
    "\n",
    "dataset = Tox21(smiles_to_g)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching a list of datapoints for dataloader.\n",
    "def collate_molgraphs(data):\n",
    "    smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "    \n",
    "    # Add self-loops to each individual graph\n",
    "    for i in range(len(graphs)):\n",
    "        graphs[i] = dgl.add_self_loop(graphs[i])\n",
    "    \n",
    "    batched_graphs = dgl.batch(graphs)\n",
    "    batched_graphs.set_n_initializer(dgl.init.zero_initializer)\n",
    "    batched_graphs.set_e_initializer(dgl.init.zero_initializer)\n",
    "\n",
    "    labels = torch.stack(labels, dim=0).to(device)\n",
    "    masks = torch.stack(masks, dim=0).to(device)\n",
    "    return smiles, batched_graphs.to(device), labels, masks\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = split_dataset(dataset, shuffle=True)\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meter(object):\n",
    "    \"\"\"Track and summarize model performance on a dataset for\n",
    "    (multi-label) binary classification.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mask = []\n",
    "        self.y_pred = []\n",
    "        self.y_true = []\n",
    "\n",
    "    def update(self, y_pred, y_true, mask):\n",
    "        \"\"\"Update for the result of an iteration\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : float32 tensor\n",
    "            Predicted molecule labels with shape (B, T),\n",
    "            B for batch size and T for the number of tasks\n",
    "        y_true : float32 tensor\n",
    "            Ground truth molecule labels with shape (B, T)\n",
    "        mask : float32 tensor\n",
    "            Mask for indicating the existence of ground\n",
    "            truth labels with shape (B, T)\n",
    "        \"\"\"\n",
    "        self.y_pred.append(y_pred.detach().cpu())\n",
    "        self.y_true.append(y_true.detach().cpu())\n",
    "        self.mask.append(mask.detach().cpu())\n",
    "\n",
    "    def roc_auc_score(self):\n",
    "        \"\"\"Compute roc-auc score for each task.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            roc-auc score for all tasks\n",
    "        \"\"\"\n",
    "        mask = torch.cat(self.mask, dim=0)\n",
    "        y_pred = torch.cat(self.y_pred, dim=0)\n",
    "        y_true = torch.cat(self.y_true, dim=0)\n",
    "        # This assumes binary case only\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        return [\n",
    "            rac(y_pred[:, i], y_true[:, i], sample_weight=mask[:, i])\n",
    "            for i in range(y_true.size(1))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_an_eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    meter = Meter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for smiles, bg, labels, masks in loader:\n",
    "            labels, masks = labels.to(device), masks.to(device)\n",
    "            labels, masks = labels.float(), masks.float()\n",
    "            logits = model(bg)\n",
    "            logits = logits.unsqueeze(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "            meter.update(logits, labels, masks)\n",
    "\n",
    "    return meter.roc_auc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(model, optimizer, loader):\n",
    "    model.train()\n",
    "    meter = Meter()\n",
    "\n",
    "    for smiles, bg, labels, masks in loader:\n",
    "        labels, masks = labels.to(device), masks.to(device)\n",
    "        labels, masks = labels.float(), masks.float()\n",
    "        logits = model(bg)\n",
    "        logits = logits.unsqueeze(-1)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        meter.update(logits, labels, masks)\n",
    "\n",
    "    return meter.roc_auc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer1(nn.Module):\n",
    "    def __init__(self, num_in_feats, num_out_feats, num_heads):\n",
    "        super(GATLayer1, self).__init__()\n",
    "        self.gat = dgl.nn.pytorch.GATConv(num_in_feats, num_out_feats, num_heads,\n",
    "                                          residual=False)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            h = self.gat(g, h)\n",
    "            return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, num_in_feats, num_out_feats, num_heads):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for _ in range(num_heads):\n",
    "            self.heads.append(GATLayer1(num_in_feats, num_out_feats, 1))\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        head_outs = [attn_head(g, h) for attn_head in self.heads]\n",
    "        # return the concatenated multi-head representation\n",
    "        return torch.cat(head_outs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT1(nn.Module):\n",
    "    def __init__(self, num_layers, num_feats, num_hidden, num_classes, num_heads):\n",
    "        super(GAT1, self).__init__()\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.gat_layers.append(MultiHeadGATLayer(num_feats, num_hidden, num_heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.gat_layers.append(MultiHeadGATLayer(num_hidden * num_heads, num_hidden, num_heads))\n",
    "        self.gat_layers.append(MultiHeadGATLayer(num_hidden * num_heads, num_classes, num_heads))\n",
    "        self.linear = nn.Linear(num_hidden * num_heads, num_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['h'].float().to(device)\n",
    "        for gat_layer in self.gat_layers:\n",
    "            h = gat_layer(g, h).to(device)\n",
    "        return h\n",
    "\n",
    "num_layers = 3\n",
    "num_feats = 74\n",
    "num_hidden = 256\n",
    "num_classes = 12\n",
    "num_heads = 2\n",
    "\n",
    "model = GAT1(num_layers, num_feats, num_hidden, num_classes, num_heads).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT1(\n",
      "  (gat_layers): ModuleList(\n",
      "    (0): MultiHeadGATLayer(\n",
      "      (heads): ModuleList(\n",
      "        (0-1): 2 x GATLayer1(\n",
      "          (gat): GATConv(\n",
      "            (fc): Linear(in_features=74, out_features=256, bias=False)\n",
      "            (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1-2): 2 x MultiHeadGATLayer(\n",
      "      (heads): ModuleList(\n",
      "        (0-1): 2 x GATLayer1(\n",
      "          (gat): GATConv(\n",
      "            (fc): Linear(in_features=512, out_features=256, bias=False)\n",
      "            (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): MultiHeadGATLayer(\n",
      "      (heads): ModuleList(\n",
      "        (0-1): 2 x GATLayer1(\n",
      "          (gat): GATConv(\n",
      "            (fc): Linear(in_features=512, out_features=12, bias=False)\n",
      "            (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "best_val_score = 0\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Add self-loops to the graph\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m bg \u001b[39m=\u001b[39m dgl\u001b[39m.\u001b[39madd_self_loop(bg)\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m trange(epochs):\n\u001b[0;32m      5\u001b[0m     train_score \u001b[39m=\u001b[39m run_a_train_epoch(model, optimizer, train_loader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bg' is not defined"
     ]
    }
   ],
   "source": [
    "# Add self-loops to the graph\n",
    "bg = dgl.add_self_loop(bg)\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    train_score = run_a_train_epoch(model, optimizer, train_loader)\n",
    "    val_score = run_an_eval_epoch(model, val_loader)\n",
    "    scheduler.step(val_score[0])\n",
    "    if val_score[0] > best_val_score:\n",
    "        best_val_score = val_score[0]\n",
    "        best_model = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model)\n",
    "test_score = run_an_eval_epoch(model, test_loader)\n",
    "print('Test ROC-AUC Score: %.4f' % test_score[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch CUDA",
   "language": "python",
   "name": "pytorch-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
