{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d82c6a8",
   "metadata": {
    "id": "2d82c6a8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "from dgllife.data import Tox21\n",
    "from dgllife.utils import SMILESToBigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer, RandomSplitter\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from dgl.data.utils import split_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score as rac\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm,trange\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68a37018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6347fd83",
   "metadata": {
    "id": "6347fd83"
   },
   "outputs": [],
   "source": [
    "smiles_to_g = SMILESToBigraph(node_featurizer=CanonicalAtomFeaturizer(), edge_featurizer=CanonicalBondFeaturizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86048dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b86048dd",
    "outputId": "518035a8-2f73-4fee-82b0-e2ee297526f8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\dhrub\\.dgl/tox21.csv.gz from https://data.dgl.ai/dataset/tox21.csv.gz...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/7831\n",
      "Processing molecule 2000/7831\n",
      "Processing molecule 3000/7831\n",
      "Processing molecule 4000/7831\n",
      "Processing molecule 5000/7831\n",
      "Processing molecule 6000/7831\n",
      "Processing molecule 7000/7831\n"
     ]
    }
   ],
   "source": [
    "dataset = Tox21(smiles_to_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2286d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb2286d8",
    "outputId": "f6277398-63f1-43ec-a928-3cc2adcb9bc2",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CCOc1ccc2nc(S(N)(=O)=O)sc2c1',\n",
       " Graph(num_nodes=16, num_edges=34,\n",
       "       ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n",
       "       edata_schemes={'e': Scheme(shape=(12,), dtype=torch.float32)}),\n",
       " tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034ed86c",
   "metadata": {
    "id": "034ed86c"
   },
   "outputs": [],
   "source": [
    "#Batching a list of datapoints for dataloader.\n",
    "def collate_molgraphs(data):\n",
    "    smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    g = dgl.batch(graphs)\n",
    "    g.set_n_initializer(dgl.init.zero_initializer)\n",
    "    g.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "    return smiles, g, labels, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f38763f0",
   "metadata": {
    "id": "f38763f0"
   },
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = split_dataset(dataset, shuffle=True)\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3c83cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4b191df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "827ba60e",
   "metadata": {
    "id": "827ba60e"
   },
   "outputs": [],
   "source": [
    "class Meter(object):\n",
    "    \"\"\"Track and summarize model performance on a dataset for\n",
    "    (multi-label) binary classification.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mask = []\n",
    "        self.y_pred = []\n",
    "        self.y_true = []\n",
    "\n",
    "    def update(self, y_pred, y_true, mask):\n",
    "        \"\"\"Update for the result of an iteration\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : float32 tensor\n",
    "            Predicted molecule labels with shape (B, T),\n",
    "            B for batch size and T for the number of tasks\n",
    "        y_true : float32 tensor\n",
    "            Ground truth molecule labels with shape (B, T)\n",
    "        mask : float32 tensor\n",
    "            Mask for indicating the existence of ground\n",
    "            truth labels with shape (B, T)\n",
    "        \"\"\"\n",
    "        self.y_pred.append(y_pred.detach().cpu())\n",
    "        self.y_true.append(y_true.detach().cpu())\n",
    "        self.mask.append(mask.detach().cpu())\n",
    "\n",
    "    def roc_auc_score(self):\n",
    "        \"\"\"Compute roc-auc score for each task.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            roc-auc score for all tasks\n",
    "        \"\"\"\n",
    "        mask = torch.cat(self.mask, dim=0)\n",
    "        y_pred = torch.cat(self.y_pred, dim=0)\n",
    "        y_true = torch.cat(self.y_true, dim=0)\n",
    "        # This assumes binary case only\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        n_tasks = y_true.shape[1]\n",
    "        scores = []\n",
    "        for task in range(n_tasks):\n",
    "            task_w = mask[:, task]\n",
    "            task_y_true = y_true[:, task][task_w != 0].numpy()\n",
    "            task_y_pred = y_pred[:, task][task_w != 0].numpy()\n",
    "            scores.append(rac(task_y_true, task_y_pred))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53908ab2",
   "metadata": {
    "id": "53908ab2"
   },
   "outputs": [],
   "source": [
    "def run_an_eval_epoch(model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, g, labels, masks = batch_data\n",
    "            atom_feats = g.ndata.pop('h')\n",
    "            bond_feats = g.edata.pop('e')\n",
    "            atom_feats, bond_feats, labels = (atom_feats.to(device), bond_feats.to(device), labels.to(device))\n",
    "            logits = model(g, atom_feats, bond_feats)\n",
    "            eval_meter.update(logits, labels, masks)\n",
    "    return np.mean(eval_meter.roc_auc_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9aa7f8c",
   "metadata": {
    "id": "d9aa7f8c"
   },
   "outputs": [],
   "source": [
    "train_ids = torch.arange(1000)\n",
    "def run_a_train_epoch(train_loader, val_loader, model):\n",
    "    loss_criterion = BCEWithLogitsLoss(pos_weight=torch.tensor(dataset.task_pos_weights(train_ids)).to(device), reduction=\"none\")\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        model.train()\n",
    "        train_meter = Meter()\n",
    "        total_loss = 0\n",
    "        for batch_id, batch_data in enumerate(train_loader):\n",
    "            smiles, g, labels, masks = batch_data\n",
    "            atom_feats = g.ndata.pop('h')\n",
    "            bond_feats = g.edata.pop('e')\n",
    "            atom_feats, bond_feats, labels, masks, g=(\n",
    "                atom_feats.to(device),\n",
    "                bond_feats.to(device),\n",
    "                labels.to(device),\n",
    "                masks.to(device),\n",
    "                g.to(device)\n",
    "            )\n",
    "            logits = model(g, atom_feats, bond_feats)\n",
    "            # Mask non-existing labels\n",
    "            loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        train_acc = run_an_eval_epoch(model, train_loader)\n",
    "        valid_acc = run_an_eval_epoch(model, val_loader)\n",
    "        print(\n",
    "            \"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n",
    "                epoch, total_loss / 10, train_acc, valid_acc\n",
    "            )\n",
    "        )\n",
    "        train_meter.update(logits, labels, masks)\n",
    "    train_score = np.mean(train_meter.roc_auc_score())\n",
    "    print(\n",
    "        \"epoch {:d}/{:d}, training roc-auc {:.4f}\".format(epoch + 1, 10 , train_score)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "513c2bf2",
   "metadata": {
    "id": "513c2bf2"
   },
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, e_feat):\n",
    "        super(GATLayer, self).__init__()\n",
    "        #self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_feat, out_feat, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_feat, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        # equation (1)\n",
    "        #self.g = g\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63f4f7d9",
   "metadata": {
    "id": "63f4f7d9"
   },
   "outputs": [],
   "source": [
    "class GATLayer1(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, e_feat):\n",
    "        super(GATLayer1, self).__init__()\n",
    "        #self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_feat+out_feat, out_feat, bias=False)\n",
    "        self.fc1 = nn.Linear(e_feat, out_feat, bias = False)\n",
    "        self.fc2 = nn.Linear(in_feat, out_feat, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_feat, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "    \n",
    "    def edge_attention1(self, edges):\n",
    "    \n",
    "        # edge UDF for equation (2)\n",
    "        catfeat = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        node_embedding=[]\n",
    "        for i in range(0, len(catfeat)):\n",
    "            u = edges.data['y'][i]\n",
    "            v = catfeat[i]\n",
    "            #res = torch.from_numpy(signal.fftconvolve(((v.cuda()).detach().cpu().clone().numpy()), (u.cuda().detach().cpu().clone().numpy()), mode='same'))\n",
    "            res = torch.as_tensor(signal.fftconvolve(cupy.asarray(v), cupy.asarray(u), mode='same'))\n",
    "            node_embedding.append(res)\n",
    "        probs = torch.stack([node_embedding[i].clone().detach().requires_grad_(True) for i in range(0,len(node_embedding))])\n",
    "        a = self.attn_fc(probs)\n",
    "        return {'f': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'f': edges.data['f']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['f'], dim=1)\n",
    "        #alpha = nodes.mailbox['f']\n",
    "        # equation (4)\n",
    "        k = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'k': k}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        # equation (1)\n",
    "        #self.g = g\n",
    "        with g.local_scope():\n",
    "            g.ndata[\"h\"] = h\n",
    "            g.edata[\"e\"] = e\n",
    "            #print(h.size())\n",
    "            z = self.fc2(h)\n",
    "            y = self.fc1(e)\n",
    "            #print(z.size())\n",
    "\n",
    "            g.ndata['z'] = z\n",
    "            g.edata['y'] = y\n",
    "        # equation (2)\n",
    "            g.apply_edges(self.edge_attention1)\n",
    "        # equation (3) & (4)\n",
    "            g.update_all(self.message_func, self.reduce_func)\n",
    "            h_N = g.ndata[\"k\"]\n",
    "            #print(h_N.size())\n",
    "            #print(h.size())\n",
    "            h_total = torch.cat([h, h_N], dim=1)\n",
    "            h_new = self.fc(h_total)\n",
    "            return [h_new, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ef9d1c4",
   "metadata": {
    "id": "9ef9d1c4"
   },
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, e_feats, num_classes, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer1(in_feats, h_feats, e_feats))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, g, in_feat, e_feat):\n",
    "        head_outs_node = [attn_head(g, in_feat, e_feat)[0] for attn_head in self.heads]\n",
    "        head_outs_edge = [attn_head(g, in_feat, e_feat)[1] for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            #print(type(head_outs_node))\n",
    "            #print(type(head_outs_edge))\n",
    "            return torch.cat(head_outs_node, dim=1), torch.cat(head_outs_edge, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ec20a29",
   "metadata": {
    "id": "3ec20a29"
   },
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, e_feats, num_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = GATLayer1(in_feats, h_feats, e_feats)\n",
    "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
    "        # multiple head outputs are concatenated together. Also, only\n",
    "        # one attention head in the output layer.\n",
    "        self.layer2 = GATLayer1(h_feats, num_classes, e_feats)\n",
    "\n",
    "    def forward(self, g, in_feat, e_feat):\n",
    "        h = self.layer1(g, in_feat, e_feat)\n",
    "        #print(h.size())\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(g, h, e_feat)\n",
    "        g.ndata[\"h\"] = h\n",
    "        return dgl.max_nodes(g, \"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4caa3493",
   "metadata": {
    "id": "4caa3493"
   },
   "outputs": [],
   "source": [
    "class GAT1(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, e_feats, num_classes, num_heads):\n",
    "        super(GAT1, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(in_feats, h_feats, e_feats,__, num_heads)\n",
    "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
    "        # multiple head outputs are concatenated together. Also, only\n",
    "        # one attention head in the output layer.\n",
    "        #self.layer2 = MultiHeadGATLayer(h_feats*num_heads, num_classes, e_feats,__,1)\n",
    "        self.layer2 = MultiHeadGATLayer(h_feats*num_heads, num_classes, h_feats*num_heads,__,1)\n",
    "\n",
    "    def forward(self, g, in_feat, e_feat):\n",
    "        #print(self.layer1(g, in_feat, e_feat)[1])\n",
    "        h, e = self.layer1(g, in_feat, e_feat)\n",
    "        #print(h.size())\n",
    "        h = F.elu(h)\n",
    "        e = F.elu(e)\n",
    "        h, e = self.layer2(g, h, e)\n",
    "        g.ndata[\"h\"] = h\n",
    "        return dgl.max_nodes(g, \"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4de6707d",
   "metadata": {
    "id": "4de6707d"
   },
   "outputs": [],
   "source": [
    "model = GAT1(\n",
    "            in_feats = 74,\n",
    "            h_feats = 60,\n",
    "            e_feats = 12,\n",
    "            num_classes = 12,\n",
    "            num_heads=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "204e9b6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "204e9b6a",
    "outputId": "c7c35517-9360-49c4-cc04-c670d93b1828",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT1(\n",
      "  (layer1): MultiHeadGATLayer(\n",
      "    (heads): ModuleList(\n",
      "      (0-1): 2 x GATLayer1(\n",
      "        (fc): Linear(in_features=134, out_features=60, bias=False)\n",
      "        (fc1): Linear(in_features=12, out_features=60, bias=False)\n",
      "        (fc2): Linear(in_features=74, out_features=60, bias=False)\n",
      "        (attn_fc): Linear(in_features=120, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): MultiHeadGATLayer(\n",
      "    (heads): ModuleList(\n",
      "      (0): GATLayer1(\n",
      "        (fc): Linear(in_features=132, out_features=12, bias=False)\n",
      "        (fc1): Linear(in_features=120, out_features=12, bias=False)\n",
      "        (fc2): Linear(in_features=120, out_features=12, bias=False)\n",
      "        (attn_fc): Linear(in_features=24, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dbc64f81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "da0f1d47f1394bf89c5b8efe85091040",
      "6577469566af49caa91a9fe74a9a60eb",
      "7acbc2cd92784c07b6ba0258aed38adf",
      "6a7b9745a71f4d83952109db7dd09a79",
      "41ed424aa5bb404f890fb2c042fea51e",
      "1e453948442b46039de6eae2d6d6bee0",
      "38304f9d042d48f2a1f4453f2e8e8d39",
      "33337d9c0d4d47c9849c2c88121117a5",
      "5b6106c169414c42bdfcb1ccef229f84",
      "2910e5c24d5845498ad25e1a42e3a5fb",
      "10e8cbf7369740f3a92ca4d8655c56fb"
     ]
    },
    "id": "dbc64f81",
    "outputId": "8c672fd4-4615-44e6-9212-fe4c15315329",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhrub\\AppData\\Local\\Temp\\ipykernel_2892\\1165395374.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_criterion = BCEWithLogitsLoss(pos_weight=torch.tensor(dataset.task_pos_weights(train_ids)).to(device), reduction=\"none\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m run_a_train_epoch(train_loader, val_loader, model)\n",
      "Cell \u001b[1;32mIn[40], line 22\u001b[0m, in \u001b[0;36mrun_a_train_epoch\u001b[1;34m(train_loader, val_loader, model)\u001b[0m\n\u001b[0;32m     14\u001b[0m bond_feats \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39medata\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m atom_feats, bond_feats, labels, masks, g\u001b[39m=\u001b[39m(\n\u001b[0;32m     16\u001b[0m     atom_feats\u001b[39m.\u001b[39mto(device),\n\u001b[0;32m     17\u001b[0m     bond_feats\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     g\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m logits \u001b[39m=\u001b[39m model(g, atom_feats, bond_feats)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Mask non-existing labels\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss \u001b[39m=\u001b[39m (loss_criterion(logits, labels) \u001b[39m*\u001b[39m (masks \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat())\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[57], line 13\u001b[0m, in \u001b[0;36mGAT1.forward\u001b[1;34m(self, g, in_feat, e_feat)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, in_feat, e_feat):\n\u001b[0;32m     12\u001b[0m     \u001b[39m#print(self.layer1(g, in_feat, e_feat)[1])\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     h, e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(g, in_feat, e_feat)\n\u001b[0;32m     14\u001b[0m     \u001b[39m#print(h.size())\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(h)\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m, in \u001b[0;36mMultiHeadGATLayer.forward\u001b[1;34m(self, g, in_feat, e_feat)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, in_feat, e_feat):\n\u001b[1;32m---> 10\u001b[0m     head_outs_node \u001b[39m=\u001b[39m [attn_head(g, in_feat, e_feat)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m attn_head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     11\u001b[0m     head_outs_edge \u001b[39m=\u001b[39m [attn_head(g, in_feat, e_feat)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m attn_head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     13\u001b[0m         \u001b[39m# concat on the output feature dimension (dim=1)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[39m#print(type(head_outs_node))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         \u001b[39m#print(type(head_outs_edge))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, in_feat, e_feat):\n\u001b[1;32m---> 10\u001b[0m     head_outs_node \u001b[39m=\u001b[39m [attn_head(g, in_feat, e_feat)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m attn_head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     11\u001b[0m     head_outs_edge \u001b[39m=\u001b[39m [attn_head(g, in_feat, e_feat)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m attn_head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     13\u001b[0m         \u001b[39m# concat on the output feature dimension (dim=1)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[39m#print(type(head_outs_node))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         \u001b[39m#print(type(head_outs_edge))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[92], line 67\u001b[0m, in \u001b[0;36mGATLayer1.forward\u001b[1;34m(self, g, h, e)\u001b[0m\n\u001b[0;32m     65\u001b[0m     g\u001b[39m.\u001b[39medata[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m y\n\u001b[0;32m     66\u001b[0m \u001b[39m# equation (2)\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     g\u001b[39m.\u001b[39;49mapply_edges(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_attention1)\n\u001b[0;32m     68\u001b[0m \u001b[39m# equation (3) & (4)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     g\u001b[39m.\u001b[39mupdate_all(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmessage_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduce_func)\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\dgl\\heterograph.py:4698\u001b[0m, in \u001b[0;36mDGLGraph.apply_edges\u001b[1;34m(self, func, edges, etype)\u001b[0m\n\u001b[0;32m   4696\u001b[0m     edata \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39minvoke_gsddmm(g, func)\n\u001b[0;32m   4697\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4698\u001b[0m     edata \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49minvoke_edge_udf(g, eid, etype, func)\n\u001b[0;32m   4700\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mnumber_of_etypes() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m etype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4701\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_e_repr(etid, eid, edata)\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\dgl\\core.py:96\u001b[0m, in \u001b[0;36minvoke_edge_udf\u001b[1;34m(graph, eid, etype, func, orig_eid)\u001b[0m\n\u001b[0;32m     87\u001b[0m dstdata \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39m_node_frames[dtid]\u001b[39m.\u001b[39msubframe(v)\n\u001b[0;32m     88\u001b[0m ebatch \u001b[39m=\u001b[39m EdgeBatch(\n\u001b[0;32m     89\u001b[0m     graph,\n\u001b[0;32m     90\u001b[0m     eid \u001b[39mif\u001b[39;00m orig_eid \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m orig_eid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     dstdata,\n\u001b[0;32m     95\u001b[0m )\n\u001b[1;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m func(ebatch)\n",
      "Cell \u001b[1;32mIn[92], line 34\u001b[0m, in \u001b[0;36mGATLayer1.edge_attention1\u001b[1;34m(self, edges)\u001b[0m\n\u001b[0;32m     32\u001b[0m     v \u001b[39m=\u001b[39m catfeat[i]\n\u001b[0;32m     33\u001b[0m     \u001b[39m#res = torch.from_numpy(signal.fftconvolve(((v.cuda()).detach().cpu().clone().numpy()), (u.cuda().detach().cpu().clone().numpy()), mode='same'))\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(signal\u001b[39m.\u001b[39mfftconvolve(cupy\u001b[39m.\u001b[39;49masarray(v), cupy\u001b[39m.\u001b[39masarray(u), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     35\u001b[0m     node_embedding\u001b[39m.\u001b[39mappend(res)\n\u001b[0;32m     36\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([node_embedding[i]\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(node_embedding))])\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\cupy\\_creation\\from_data.py:76\u001b[0m, in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39masarray\u001b[39m(a, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     50\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Converts an object to array.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[39m    This is equivalent to ``array(a, dtype, copy=False)``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m _core\u001b[39m.\u001b[39;49marray(a, dtype, \u001b[39mFalse\u001b[39;49;00m, order)\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:2381\u001b[0m, in \u001b[0;36mcupy._core.core.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:2405\u001b[0m, in \u001b[0;36mcupy._core.core.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:2530\u001b[0m, in \u001b[0;36mcupy._core.core._array_default\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "run_a_train_epoch(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d202f41d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d202f41d",
    "outputId": "66e9214a-7f99-464c-910e-febabab2ea91",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "Cannot assign node feature \"h\" on device cuda:0 to a graph on device cpu. Call DGLGraph.to() to copy the graph to the same device.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_score \u001b[39m=\u001b[39m run_an_eval_epoch(model, test_loader)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest score \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(test_score))\n",
      "Cell \u001b[1;32mIn[31], line 10\u001b[0m, in \u001b[0;36mrun_an_eval_epoch\u001b[1;34m(model, data_loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m         bond_feats \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39medata\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m         atom_feats, bond_feats, labels \u001b[39m=\u001b[39m (atom_feats\u001b[39m.\u001b[39mto(device), bond_feats\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m---> 10\u001b[0m         logits \u001b[39m=\u001b[39m model(g, atom_feats, bond_feats)\n\u001b[0;32m     11\u001b[0m         eval_meter\u001b[39m.\u001b[39mupdate(logits, labels, masks)\n\u001b[0;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(eval_meter\u001b[39m.\u001b[39mroc_auc_score())\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[57], line 13\u001b[0m, in \u001b[0;36mGAT1.forward\u001b[1;34m(self, g, in_feat, e_feat)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, in_feat, e_feat):\n\u001b[0;32m     12\u001b[0m     \u001b[39m#print(self.layer1(g, in_feat, e_feat)[1])\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     h, e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(g, in_feat, e_feat)\n\u001b[0;32m     14\u001b[0m     \u001b[39m#print(h.size())\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(h)\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m, in \u001b[0;36mMultiHeadGATLayer.forward\u001b[1;34m(self, g, in_feat, e_feat)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, in_feat, e_feat):\n\u001b[1;32m---> 10\u001b[0m     head_outs_node \u001b[39m=\u001b[39m [attn_head(g, in_feat, e_feat)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m attn_head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     11\u001b[0m     head_outs_edge \u001b[39m=\u001b[39m [attn_head(g, in_feat, e_feat)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m attn_head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     13\u001b[0m         \u001b[39m# concat on the output feature dimension (dim=1)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[39m#print(type(head_outs_node))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         \u001b[39m#print(type(head_outs_edge))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, in_feat, e_feat):\n\u001b[1;32m---> 10\u001b[0m     head_outs_node \u001b[39m=\u001b[39m [attn_head(g, in_feat, e_feat)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m attn_head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     11\u001b[0m     head_outs_edge \u001b[39m=\u001b[39m [attn_head(g, in_feat, e_feat)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m attn_head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     13\u001b[0m         \u001b[39m# concat on the output feature dimension (dim=1)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[39m#print(type(head_outs_node))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         \u001b[39m#print(type(head_outs_edge))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[59], line 56\u001b[0m, in \u001b[0;36mGATLayer1.forward\u001b[1;34m(self, g, h, e)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, h, e):\n\u001b[0;32m     53\u001b[0m     \u001b[39m# equation (1)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[39m#self.g = g\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[39mwith\u001b[39;00m g\u001b[39m.\u001b[39mlocal_scope():\n\u001b[1;32m---> 56\u001b[0m         g\u001b[39m.\u001b[39mndata[\u001b[39m\"\u001b[39m\u001b[39mh\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m h\n\u001b[0;32m     57\u001b[0m         g\u001b[39m.\u001b[39medata[\u001b[39m\"\u001b[39m\u001b[39me\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m e\n\u001b[0;32m     58\u001b[0m         \u001b[39m#print(h.size())\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\dgl\\view.py:99\u001b[0m, in \u001b[0;36mHeteroNodeDataView.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(val, \u001b[39mdict\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m, (\n\u001b[0;32m     96\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe HeteroNodeDataView has only one node type. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mplease pass a tensor directly\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     )\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49m_set_n_repr(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ntid, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nodes, {key: val})\n",
      "File \u001b[1;32mc:\\Users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\dgl\\heterograph.py:4347\u001b[0m, in \u001b[0;36mDGLGraph._set_n_repr\u001b[1;34m(self, ntid, u, data)\u001b[0m\n\u001b[0;32m   4342\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(\n\u001b[0;32m   4343\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpect number of features to match number of nodes (len(u)).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4344\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (nfeats, num_nodes)\n\u001b[0;32m   4345\u001b[0m     )\n\u001b[0;32m   4346\u001b[0m \u001b[39mif\u001b[39;00m F\u001b[39m.\u001b[39mcontext(val) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice:\n\u001b[1;32m-> 4347\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(\n\u001b[0;32m   4348\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mCannot assign node feature \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m on device \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to a graph on\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   4349\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m device \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. Call DGLGraph.to() to copy the graph to the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4350\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m same device.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(key, F\u001b[39m.\u001b[39mcontext(val), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   4351\u001b[0m     )\n\u001b[0;32m   4352\u001b[0m \u001b[39m# To prevent users from doing things like:\u001b[39;00m\n\u001b[0;32m   4353\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   4354\u001b[0m \u001b[39m#     g.pin_memory_()\u001b[39;00m\n\u001b[0;32m   4355\u001b[0m \u001b[39m#     g.ndata['x'] = torch.randn(...)\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[39m#     sg = g.sample_neighbors(torch.LongTensor([...]).cuda())\u001b[39;00m\n\u001b[0;32m   4357\u001b[0m \u001b[39m#     sg.ndata['x']    # Becomes a CPU tensor even if sg is on GPU due to lazy slicing\u001b[39;00m\n\u001b[0;32m   4358\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   4359\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_pinned()\n\u001b[0;32m   4360\u001b[0m     \u001b[39mand\u001b[39;00m F\u001b[39m.\u001b[39mcontext(val) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4361\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m F\u001b[39m.\u001b[39mis_pinned(val)\n\u001b[0;32m   4362\u001b[0m ):\n",
      "\u001b[1;31mDGLError\u001b[0m: Cannot assign node feature \"h\" on device cuda:0 to a graph on device cpu. Call DGLGraph.to() to copy the graph to the same device."
     ]
    }
   ],
   "source": [
    "test_score = run_an_eval_epoch(model, test_loader)\n",
    "print(\"Test score {:.4f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bfea56c",
   "metadata": {
    "id": "8bfea56c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (8.0.6)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipywidgets) (6.19.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipywidgets) (8.12.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.1.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: backcall in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (3.11.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.5.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (305.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dhrub\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10e8cbf7369740f3a92ca4d8655c56fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e453948442b46039de6eae2d6d6bee0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2910e5c24d5845498ad25e1a42e3a5fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33337d9c0d4d47c9849c2c88121117a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38304f9d042d48f2a1f4453f2e8e8d39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41ed424aa5bb404f890fb2c042fea51e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b6106c169414c42bdfcb1ccef229f84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6577469566af49caa91a9fe74a9a60eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e453948442b46039de6eae2d6d6bee0",
      "placeholder": "​",
      "style": "IPY_MODEL_38304f9d042d48f2a1f4453f2e8e8d39",
      "value": "100%"
     }
    },
    "6a7b9745a71f4d83952109db7dd09a79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2910e5c24d5845498ad25e1a42e3a5fb",
      "placeholder": "​",
      "style": "IPY_MODEL_10e8cbf7369740f3a92ca4d8655c56fb",
      "value": " 2/2 [19:29&lt;00:00, 583.53s/it]"
     }
    },
    "7acbc2cd92784c07b6ba0258aed38adf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33337d9c0d4d47c9849c2c88121117a5",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5b6106c169414c42bdfcb1ccef229f84",
      "value": 2
     }
    },
    "da0f1d47f1394bf89c5b8efe85091040": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6577469566af49caa91a9fe74a9a60eb",
       "IPY_MODEL_7acbc2cd92784c07b6ba0258aed38adf",
       "IPY_MODEL_6a7b9745a71f4d83952109db7dd09a79"
      ],
      "layout": "IPY_MODEL_41ed424aa5bb404f890fb2c042fea51e"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
