{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: partially initialized module 'pdb' has no attribute 'Pdb' (most likely due to a circular import). Did you mean: 'bdb'?. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "from dgllife.data import Tox21\n",
    "from dgllife.utils import SMILESToBigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer, RandomSplitter\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from dgl.data.utils import split_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score as rac\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from scipy import signal\n",
    "smiles_to_g = SMILESToBigraph(node_featurizer=CanonicalAtomFeaturizer(), edge_featurizer=CanonicalBondFeaturizer())\n",
    "dataset = Tox21(smiles_to_g)\n",
    "dataset[0]\n",
    "\n",
    "\n",
    "# Batching a list of datapoints for dataloader.\n",
    "def collate_molgraphs(data):\n",
    "    smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    g = dgl.batch(graphs)\n",
    "    g.set_n_initializer(dgl.init.zero_initializer)\n",
    "    g.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "    return smiles, g, labels, masks\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = split_dataset(dataset, shuffle=True)\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True, collate_fn=collate_molgraphs)\n",
    "\n",
    "\n",
    "class Meter(object):\n",
    "    \"\"\"Track and summarize model performance on a dataset for\n",
    "    (multi-label) binary classification.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mask = []\n",
    "        self.y_pred = []\n",
    "        self.y_true = []\n",
    "\n",
    "    def update(self, y_pred, y_true, mask):\n",
    "        \"\"\"Update for the result of an iteration\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : float32 tensor\n",
    "            Predicted molecule labels with shape (B, T),\n",
    "            B for batch size and T for the number of tasks\n",
    "        y_true : float32 tensor\n",
    "            Ground truth molecule labels with shape (B, T)\n",
    "        mask : float32 tensor\n",
    "            Mask for indicating the existence of ground\n",
    "            truth labels with shape (B, T)\n",
    "        \"\"\"\n",
    "        self.y_pred.append(y_pred.detach().cpu())\n",
    "        self.y_true.append(y_true.detach().cpu())\n",
    "        self.mask.append(mask.detach().cpu())\n",
    "\n",
    "    def roc_auc_score(self):\n",
    "        \"\"\"Compute roc-auc score for each task.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            roc-auc score for all tasks\n",
    "        \"\"\"\n",
    "        mask = torch.cat(self.mask, dim=0)\n",
    "        y_pred = torch.cat(self.y_pred, dim=0)\n",
    "        y_true = torch.cat(self.y_true, dim=0)\n",
    "        # This assumes binary case only\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        n_tasks = y_true.shape[1]\n",
    "        scores = []\n",
    "        for task in range(n_tasks):\n",
    "            task_wise_mask = mask[:, task] > 0.5\n",
    "            task_wise_y_true = y_true[:, task][task_wise_mask]\n",
    "            if len(torch.unique(task_wise_y_true)) == 1:\n",
    "                scores.append(np.nan)\n",
    "                continue\n",
    "            task_wise_y_pred = y_pred[:, task][task_wise_mask]\n",
    "            scores.append(rac(task_wise_y_true, task_wise_y_pred))\n",
    "        return scores\n",
    "\n",
    "\n",
    "class GATLayer1(nn.Module):\n",
    "    \"\"\"Single GAT layer implementation\"\"\"\n",
    "\n",
    "    def __init__(self, in_feats, out_feats, num_heads, activation):\n",
    "        super(GATLayer1, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.heads = nn.ModuleList()\n",
    "        for _ in range(num_heads):\n",
    "            self.heads.append(self.build_head(in_feats, out_feats))\n",
    "        self.out_feats = out_feats\n",
    "        self.activation = activation\n",
    "\n",
    "    def build_head(self, in_feats, out_feats):\n",
    "        return nn.Linear(in_feats, out_feats, bias=False)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : dgl.DGLGraph\n",
    "            DGLGraph for a batch of graphs\n",
    "        h : torch.Tensor\n",
    "            Node features with shape (B, N, D), where B for the batch size,\n",
    "            N for the number of nodes, D for the number of node features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            New node features with shape (B, N, H), where H for the output\n",
    "            node feature size\n",
    "        \"\"\"\n",
    "        if self.num_heads > 1:\n",
    "            hs = []\n",
    "            for head in self.heads:\n",
    "                hs.append(head(g, h).unsqueeze(0))\n",
    "            hs = torch.cat(hs, dim=0)\n",
    "            h = torch.mean(hs, dim=0)\n",
    "        else:\n",
    "            h = self.heads[0](g, h)\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class MultiHeadGATLayer(nn.Module):\n",
    "    \"\"\"Multiple GAT layer implementation\"\"\"\n",
    "\n",
    "    def __init__(self, in_feats, out_feats, num_heads, activation, feat_drop, attn_drop, negative_slope, residual):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.residual = residual\n",
    "        self.num_heads = num_heads\n",
    "        for i in range(num_heads):\n",
    "            self.gat_layers.append(GATLayer1(in_feats, out_feats, 1, activation))\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.negative_slope = negative_slope\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : dgl.DGLGraph\n",
    "            DGLGraph for a batch of graphs\n",
    "        h : torch.Tensor\n",
    "            Node features with shape (B, N, D), where B for the batch size,\n",
    "            N for the number of nodes, D for the number of node features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            New node features with shape (B, N, H), where H for the output\n",
    "            node feature size\n",
    "        \"\"\"\n",
    "        # (B, N, D) -> (B, N, H)\n",
    "        head_outs = []\n",
    "        for l in self.gat_layers:\n",
    "            head_outs.append(l(g, h))\n",
    "        h = torch.cat(head_outs, dim=-1)\n",
    "        if self.residual:\n",
    "            h = h + self.feat_drop(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class GAT1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 activation=F.elu,\n",
    "                 feat_drop=0.6,\n",
    "                 attn_drop=0.6,\n",
    "                 negative_slope=0.2,\n",
    "                 residual=False):\n",
    "        super(GAT1, self).__init__()\n",
    "        self.g = g\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        self.gat_layers.append(MultiHeadGATLayer(in_dim, num_hidden, num_heads[0], activation,\n",
    "                                                 feat_drop, attn_drop, negative_slope, residual))\n",
    "        for l in range(1, num_layers):\n",
    "            self.gat_layers.append(MultiHeadGATLayer(num_hidden * num_heads[l-1],\n",
    "                                                     num_hidden, num_heads[l], activation,\n",
    "                                                     feat_drop, attn_drop, negative_slope, residual))\n",
    "        self.gat_layers.append(MultiHeadGATLayer(num_hidden * num_heads[-2],\n",
    "                                                 num_classes, num_heads[-1], None,\n",
    "                                                 feat_drop, attn_drop, negative_slope, residual))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](self.g, h).flatten(1)\n",
    "        # output projection\n",
    "        logits = self.gat_layers[-1](self.g, h).mean(1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GAT1(train_set.g,\n",
    "             num_layers=3,\n",
    "             in_dim=train_set.g.ndata['feat'].shape[-1],\n",
    "             num_hidden=128,\n",
    "             num_classes=12,\n",
    "             num_heads=[4, 4, 6],\n",
    "             activation=F.elu,\n",
    "             feat_drop=0.6,\n",
    "             attn_drop=0.6,\n",
    "             negative_slope=0.2,\n",
    "             residual=True).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = BCEWithLogitsLoss(pos_weight=train_set.pos_weights.to(device))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "patience = 10\n",
    "early_stopping = EarlyStopping(patience=patience)\n",
    "best_val_score = -1\n",
    "best_model_path = \"best_model.pt\"\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_meter = Meter()\n",
    "\n",
    "    for batch_id, (_, bg, label, mask) in enumerate(train_loader):\n",
    "        bg = bg.to(device)\n",
    "        label = label.to(device)\n",
    "        mask = mask.to(device)\n",
    "        atom_feats = bg.ndata.pop('feat').to(device)\n",
    "        logits = model(atom_feats)\n",
    "        loss = criterion(logits, label.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_meter.update(logits, label, mask)\n",
    "\n",
    "    model.eval()\n",
    "    val_score = np.mean(train_meter.roc_auc_score())\n",
    "    if val_score > best_val_score:\n",
    "        best_val_score = val_score\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stopping(val_score)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "    model.train()\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "test_meter = Meter()\n",
    "for _, bg, label, mask in test_loader:\n",
    "    bg = bg.to(device)\n",
    "    label = label.to(device)\n",
    "    mask = mask.to(device)\n",
    "    atom_feats = bg.ndata.pop('feat').to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(atom_feats)\n",
    "    test_meter.update(logits, label, mask)\n",
    "\n",
    "test_score = np.mean(test_meter.roc_auc_score())\n",
    "print(f\"Test ROC-AUC score: {test_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
